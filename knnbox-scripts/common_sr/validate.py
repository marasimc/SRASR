import logging
import os
import sys
from itertools import chain

import sys
sys.path.append('../../.')
import torch
from fairseq import checkpoint_utils, distributed_utils, options, utils

## knnbox related code start >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
from knnbox.datastore import Datastore, GreedyMergeDatastore, PckDatastore
from knnbox.common_utils import filter_pad_tokens, global_vars
import numpy as np
## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< end

## symbolic regression related code start >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
from symbolicregression.slurm import init_signal_handler, init_distributed_mode
from symbolicregression.envs import build_env
from symbolicregression.model import build_modules
from symbolicregression.model import check_model_params, build_modules
from symbolicregression.parsers import get_parser
from symbolicregression.trainer import Trainer
## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< end

logging.basicConfig(
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=os.environ.get("LOGLEVEL", "INFO").upper(),
    stream=sys.stdout,
)
logger = logging.getLogger("fairseq_cli.validate")

## symbolic regression related code start >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
def get_params():
    parser = get_parser()
    params = parser.parse_args([])
    
    # check parameters
    check_model_params(params)
    
    return params
## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< end

def main(args, override_args=None, model_path='../../symbolicregression_utils/weights/model1.pt'):
    utils.import_user_module(args)

    assert (
        args.max_tokens is not None or args.batch_size is not None
    ), "Must specify batch size either with --max-tokens or --batch-size"

    use_fp16 = args.fp16
    use_cuda = torch.cuda.is_available() and not args.cpu

    if use_cuda:
        torch.cuda.set_device(args.device_id)

    if not use_cuda:
        model = torch.load(model_path, map_location=torch.device('cpu'))
    else:
        model = torch.load(model_path).cuda()
    sr_params = get_params()
    init_distributed_mode(sr_params)
    if sr_params.is_slurm_job:
        init_signal_handler()
    
    # # Move models to GPU
    # for model in models:
    #     if use_fp16:
    #         model.half()
    #     if use_cuda:
    #         model.cuda()

    # # Print args
    # logger.info(model_args)

    # # Build criterion
    # criterion = task.build_criterion(model_args)
    # criterion.eval()

    
    ## knnbox related code start >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    knn_type = 'enhance_knn_sr'
    if "datastore" not in global_vars():
        # create suitable datastore class if not exists
        if knn_type in ["vanilla_knn_mt", "adaptive_knn_mt", "kernel_smoothed_knn_mt", "vanilla_knn_mt_visual", "robust_knn_mt", "enhance_knn_sr"]:
            global_vars()["optor_datastore"] = Datastore(path=args.knn_datastore_path+'/optor')
            global_vars()["const_datastore"] = Datastore(path=args.knn_datastore_path+'/const')
        if knn_type == "greedy_merge_knn_mt":
            global_vars()["datastore"] = GreedyMergeDatastore(path=args.knn_datastore_path)
        if knn_type == "pck_knn_mt":
            global_vars()["datastore"] = PckDatastore(
                path=args.knn_datastore_path,
                reduction_network_input_dim=args.decoder_embed_dim,
                reduction_network_output_dim=args.knn_reduct_dim,
                dictionary_len=len(task.tgt_dict),
                )
            
    optor_datastore = global_vars()["optor_datastore"]
    const_datastore = global_vars()["const_datastore"]
    ## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< end

    ## symbolic regression related code start >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    # compared to the original train process in end2end, we don't need to train the model here, thus we just process for one epoch
    env = build_env(sr_params)
    # env.rng = np.random.RandomState(0)      # ！！！must delete this line, otherwise the data generated by dataloader in each iteration will be the same
    modules = build_modules(env, sr_params)     # can not be deleted, because it will define get_length_after_batching parameter in env
    task = 'functions'
    
    # create dataloader
    sr_params.batch_size = 8            # TODO: change batch size here
    if sr_params.env_base_seed < 0:
        sr_params.env_base_seed = np.random.randint(1_000_000_000)
    # reload_data = "functions,/workspace/knn-SR/knn-SR/knnbox-scripts/robust-knn-sr/checkpoints_2025-04-23-08-34-18/data.prefix,,"
    # s = [x.split(",") for x in reload_data.split(";") if len(x) > 0]
    # data_path = {
    #             task: (
    #                 train_path if train_path != "" else None,
    #                 valid_path if valid_path != "" else None,
    #                 test_path if test_path != "" else None,
    #             )
    #             for task, train_path, valid_path, test_path in s
    #         }
    data_path=None
    dataloader = {
        task: iter(env.create_train_iterator(task, data_path, sr_params))
    }
    
    sr_params.n_steps_per_epoch = 30000 # TODO: change n_steps_per_epoch here
    inner_epoch = 0
    while inner_epoch < sr_params.n_steps_per_epoch:
        # 1. get each module
        embedder = model.embedder
        encoder = model.encoder
        decoder = model.decoder
        embedder.eval()
        encoder.eval()
        decoder.eval()
        
        # 2. get batch
        try:
            samples, errors = next(dataloader[task])
        except Exception as e:
            print('error: ', e)
            logger.error(
                "An unknown exception of type {0} occurred in line {1} when fetching batch. "
                "Arguments:{2!r}. Restarting ...".format(
                    type(e).__name__, sys.exc_info()[-1].tb_lineno, e.args
                )
            )
            raise
        
        # 3. embed the input
        x_to_fit = samples["x_to_fit"]      # (bs, num_of_points, num_X)
        y_to_fit = samples["y_to_fit"]      # (bs, num_of_points, num_y)

        x1 = []
        for seq_id in range(len(x_to_fit)):
            x1.append([])
            for seq_l in range(len(x_to_fit[seq_id])):
                x1[seq_id].append([x_to_fit[seq_id][seq_l], y_to_fit[seq_id][seq_l]])

        x1, len1 = embedder(x1)         # x1->(num_of_points, bs, embed_dim)；len1=(bs)

        if sr_params.use_skeleton:      # sr_params.use_skeleton=False
            x2, len2 = env.batch_equations(
                env.word_to_idx(
                    samples["skeleton_tree_encoded"], float_input=False
                )
            )
        else:
            x2, len2 = env.batch_equations(         # x2->(max_len, bs); len2=(bs)
                env.word_to_idx(samples["tree_encoded"], float_input=False)
            )

        alen = torch.arange(sr_params.max_src_len, dtype=torch.long, device=len2.device) 
        pred_mask = (alen[:, None] < len2[None] - 1)    # (slen, bs)
        
        y = x2[1:].masked_select(pred_mask[:-1])    # (pred_mask.sum(),)
        assert len(y) == (len2 - 1).sum().item()   
        x2, len2, y = x2.cuda(), len2.cuda(), y.cuda()

        # 4. save values to datastore
        val = 90   # index > 90 are constants
        optor_range_mask = pred_mask[:-1] & ((x2[1:] <= val).to(pred_mask.device))
        optor_range_mask = torch.cat([optor_range_mask, pred_mask[-1, None]], dim=0)
        const_range_mask = pred_mask[:-1] & ((x2[1:] > val).to(pred_mask.device))
        const_range_mask = torch.cat([const_range_mask, pred_mask[-1, None]], dim=0)
        optor_tokens = x2[1:].masked_select(optor_range_mask[:-1].to(x2.device))
        const_tokens = x2[1:].masked_select(const_range_mask[:-1].to(x2.device))
        
        if knn_type in ["enhance_knn_sr"]:
            # non_pad_tokens, mask = y, pred_mask
            optor_datastore["vals"].add(optor_tokens)
            const_datastore["vals"].add(const_tokens)
            # datastore.set_pad_mask(mask)    
        else:
            raise NotImplementedError

        # 4. forward pass to compute the keys, and store them into datastore
        if sr_params.amp == -1 or sr_params.nvidia_apex:
            # print('run here... ')
            encoded = encoder("fwd", x=x1, lengths=len1, causal=False)
            decoded = decoder(
                "fwd",
                x=x2,
                lengths=len2,
                causal=True,
                src_enc=encoded.transpose(0, 1),
                src_len=len1,
            )   # the last hidden state of the decoder  # [max_src_len, bs, dim]
            # _, loss = decoder(
            #     "predict", tensor=decoded, pred_mask=pred_mask, y=y, get_scores=False
            # )     
            # keys = decoded[pred_mask.unsqueeze(-1).expand_as(decoded)].view(-1, decoder.dim)
            # datastore["keys"].add(keys.half())
            optor_keys = decoded[optor_range_mask.unsqueeze(-1).expand_as(decoded)].view(-1, decoder.dim)
            optor_datastore["keys"].add(optor_keys.half())
            const_keys = decoded[const_range_mask.unsqueeze(-1).expand_as(decoded)].view(-1, decoder.dim)
            const_datastore["keys"].add(const_keys.half())
            assert len(optor_tokens) == optor_keys.size(0), "optor tokens and keys size mismatch, {} vs {}".format(len(optor_tokens), optor_keys.size(0))
            assert len(const_tokens) == const_keys.size(0), "const tokens and keys size mismatch, {} vs {}".format(len(const_tokens), const_keys.size(0))
            
            # f.write(str(inner_epoch) +'\n')
            # f.write(str(samples) +'\n')
            # if tensor_hash(optor_keys.half()) in stored_hashes:
            #     print("optor_keys already in stored_hashes")
            #     print(samples["tree_encoded"])
            #     print("optor_keys: ", optor_keys.half())
            #     break
            # else:
            #     stored_hashes.add(tensor_hash(optor_keys.half()))
        else:
            with torch.cuda.amp.autocast():
                encoded = encoder("fwd", x=x1, lengths=len1, causal=False)
                decoded = decoder(
                    "fwd",
                    x=x2,
                    lengths=len2,
                    causal=True,
                    src_enc=encoded.transpose(0, 1),
                    src_len=len1,
                )
                # _, loss = decoder(
                #     "predict",
                #     tensor=decoded,
                #     pred_mask=pred_mask,
                #     y=y,
                #     get_scores=False,
                # )
                # keys = decoded[pred_mask.unsqueeze(-1).expand_as(decoded)].view(-1, decoder.dim)
                # datastore["keys"].add(keys.half())
        
        # 5. record the inner epoch
        inner_epoch += 1
    
    # f.close()
    ## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< end


    ## knnbox related code start >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    # release memory to make sure we have enough gpu memory to build faiss index
    del model, task, dataloader
    if use_cuda:
        torch.cuda.empty_cache()    # release gpu memory

    if knn_type in ["enhance_knn_sr"]:
        optor_datastore.dump()    # dump to disk
        optor_datastore.build_faiss_index("keys", use_gpu=(not args.build_faiss_index_with_cpu))   # build faiss index
        const_datastore.dump()    # dump to disk
        const_datastore.build_faiss_index("keys", use_gpu=(not args.build_faiss_index_with_cpu))   # build faiss index
    elif knn_type == "greedy_merge_knn_mt":
        assert NotImplementedError, "greedy_merge_knn_mt is not implemented yet"
        
    elif knn_type == "pck_knn_mt":
        pass
        # datastore.dump() # dump the un-pruned datastore to disk

    ## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< end


## knnbox code start >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
def get_build_datastore_parser(default_task=None):
    r"""
    very similar to options.get_validation_parser() but parse arch as well.

    Difference:
    - when validate, we don't need to specify --arch and model args, because they are
    recorded in .pt file.

    - when building datastore, we need to load the saved model parameter to a knn-mt arch,
    which is different from the checkpoint original arch.
    For example, I have a nmt checkpoint with arch `transformer_iwslt_de_en`, and now I want to
    load it's parameter to arch `vanilla@transformer_iwslt_de_en`, I must specify
    arch = "vanilla@transfromer_iwslt_de_en".
    """
    parser = options.get_parser("Validation", default_task)
    options.add_dataset_args(parser, train=True)
    options.add_distributed_training_args(parser, default_world_size=1)
    # knnbox add one line below to parse arch
    options.add_model_args(parser)
    group = parser.add_argument_group("Evaluation")
    from fairseq.dataclass.data_class import CommonEvalParams
    options.gen_parser_from_dataclass(group, CommonEvalParams())
    return parser
## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< end 

def cli_main():
    ## knnbox related code start >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    # parser = options.get_validation_parser()
    parser = get_build_datastore_parser()
    args = options.parse_args_and_arch(parser)


    ## only override args that are explicitly given on the command line
    # override_parser = options.get_validation_parser()
    override_parser = get_build_datastore_parser()
    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< end
    override_args = options.parse_args_and_arch(override_parser, suppress_defaults=True)

    distributed_utils.call_main(args, main, override_args=override_args)


import hashlib
def tensor_hash(t):
    return hashlib.sha256(t.detach().cpu().numpy().tobytes()).hexdigest()

if __name__ == "__main__":
    cli_main()
    

