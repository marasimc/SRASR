:<<!
[script description]: train enhance-knn-sr's meta-k network and confidence network
[dataset]: data generated by End2End
[base model]: End2End

note 1. You can adjust --batch-size and --update-freq based on your GPU memory.
original paper recommand that batch-size*update-freq equals 32.
!
# this line speed up faiss
export OMP_WAIT_POLICY=PASSIVE

PROJECT_PATH=$( cd -- "$( dirname -- "$ BASH_SOURCE[0]}" )" &> /dev/null && pwd )/../..
BASE_MODEL=symbolicregression/weights/model1.pt
DATA_PATH=data

OPTOR_MAX_K=4
CONST_MAX_K=64
BS=64
SAVE_DIR="$PROJECT_PATH/save-models/combiner/enhanced-adaptive/end2end_max-k-${OPTOR_MAX_K}-${CONST_MAX_K}_bs${BS}_learnt_3w-data"
DATASTORE_LOAD_PATH=$PROJECT_PATH/datastore/symbolicregression/end2end_epoch-30000
MAX_K=8 


CUDA_VISIBLE_DEVICES=3 python $PROJECT_PATH/knnbox-scripts/common_sr/train.py $DATA_PATH \
--task translation \
--train-subset valid --valid-subset valid \
--best-checkpoint-metric "loss" \
--finetune-from-model $BASE_MODEL \
--optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-8 --clip-norm 1.0 \
--lr 3e-4 --lr-scheduler reduce_lr_on_plateau \
--min-lr 3e-05 --label-smoothing 0.001 \
--lr-patience 5 --lr-shrink 0.5 --patience 30 --max-epoch 500 --max-update 5000 --validate-after-updates 1000 \
--criterion label_smoothed_cross_entropy \
--save-interval-updates 100 \
--no-epoch-checkpoints --no-last-checkpoints --no-save-optimizer-state \
--tensorboard-logdir $SAVE_DIR/log \
--save-dir $SAVE_DIR \
--batch-size 4 \
--update-freq 8 \
--user-dir $PROJECT_PATH/knnbox/models \
--arch "adaptive_knn_mt@transformer_wmt19_de_en" \
--knn-mode "train_metak" \
--knn-datastore-path $DATASTORE_LOAD_PATH \
--knn-max-k $MAX_K \
--knn-k-type trainable \
--knn-lambda-type trainable \
--knn-temperature-type trainable \
--knn-combiner-path $SAVE_DIR \
--optor_max_k $OPTOR_MAX_K \
--const_max_k $CONST_MAX_K \
